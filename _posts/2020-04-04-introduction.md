---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg "Jonas Schuett")

## About

### AI governance researcher • Policy advisor • Lawyer

I'm a Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/) in Oxford. Within the policy team, I lead the workstream on risk management and corporate governance.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s public policy team. I also helped found the [Institute for Law and AI (LawAI](https://law-ai.org/), where I'm still a board member.

I’m currently wrapping up my PhD in law at Goethe University Frankfurt. I hold a law degree from Heidelberg University, and have studied economics at the University of Zurich.

I live in Berlin, but come to Oxford every four to six weeks. I‘m also regularly in San Francisco and in Washington D.C.

[Twitter](https://twitter.com/jonasschuett) • [LinkedIn](https://www.linkedin.com/in/jonasschuett) • [Email](mailto:jonas.schuett@governance.ai).

## Research

My research is aimed at reducing societal risks from AI. I’m particularly concerned about emerging risks from frontier AI models.

I try to inform high-stakes governance decisions by policymakers (e.g. the US and UK government) and frontier AI developers (e.g. OpenAI, Google DeepMind, and Anthropic).

This includes decisions about risk management practices, regulation, corporate governance, and auditing.

### Risk management
It becomes increasingly clear that the development and deployment of frontier AI models poses severe risks to society. I study how these risks can be measured and reduced to an acceptable level.

Ongoing research projects:

Delphi study
Risk estimation
Risk thresholds
Safeguard assessment
Passing judgment on RSPs
RSPs+

Previous research projects:

**Risk management in the AI Act.** The EU AI Act is the most comprehensive attempt to regulate AI in a major jurisdiction. In this article, I conduct a legal analysis of Art. 9, the key risk management provision.

**Risk assessments at AGI companies** (with Leonie Koessler). We review popular risk assessment techniques from other industries and discuss how companies that have the stated goal of build AGI could use them.

[Figure]

### Regulation

Although self-governance plays an important role in reducing emerging risks from AI, we will eventually need frontier AI regulation. I‘m working on several research projects intended to inform the design of a regulatory regime for frontier AI models, especially in the UK and US.

Ongoing research projects:
Rules-based vs. outcomes-based
Safety case

Previous research projects:

**Frontier AI regulation** (with Markus Anderljung and others). This multi-author report is the first attempt to sketch a regulatory regime for frontier AI models.

**Defining the scope of AI regulations.** In this article, I argue that the material scope of AI regulation should not rely on the term AI. Instead, policy makers should take a risk-based approach. They should identify the features of AI systems that drive key risks, and then define these features. These features might be related to the system’s design, application, or capabilities.

[Figure]

### Corporate governance

As frontier AI developers transition from startups to more mature companies, they need to improve their corporate governance. I‘m particularly interested in ways to improve their risk governance.

Ongoing research projects:
Board governance

Previous research projects:

**How to design an AI ethics board** (with Anka Reuel and Alexis Carlier). We identify key design choices and discuss how each of them affects the board‘s ability to reduce societal risks from AI.

3LoD
Internal audit
Corporate governance

[Figure]

### Auditing

The development and deployment of frontier AI models has significant effects on society. Key governance decisions should therefore not be left to the developers; society needs to have a say. At the minimum, society needs to be informed. A common way to gather information about a model and the governance of its developers is auditing.

Previous research projects:
Auditing LLMs
External scrutiny
AI certification

[Figure]

## Publications

Most of my research gets published in peer-reviewed journals, but I also upload preprints to arXiv. In addition to academic articles, I write policy briefs, typically in response to a request for information.

### Research
- **Schuett, J.**, Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. _AI and Ethics_. <https://doi.org/10.1007/s43681-023-00409-y>

- Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. (2023). _Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework_. arXiv. <https://arxiv.org/abs/2311.14711>

- **Schuett, J.** (2023). Three lines of defense against risks from AI. _AI & Society_. <https://doi.org/10.1007/s00146-023-01811-0>

- Alaga, J., & **Schuett, J.** (2023). _Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers_. arXiv. <https://arxiv.org/abs/2310.00374>

- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., **Schuett, J.**, Wei, K., Winter, C., Arnold, M., ÓhÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., Levermore, P., Hazell, J., & Gupta, A. (2023). _Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives_. arXiv. <https://arxiv.org/abs/2311.09227>

- Koessler, L., & **Schuett, J.** (2023). _Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries_. arXiv. <https://arxiv.org/abs/2307.08823>

- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, F., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., **Schuett, J.**, Shavit, Y., Siddarth, D., Trager, R., & Wolf, K. (2023). _Frontier AI regulation: Managing emerging risks to public safety_. arXiv. <https://arxiv.org/abs/2307.03718>

- **Schuett, J.** (2023). _AGI labs need an internal audit function_. arXiv. <https://arxiv.org/abs/2305.17038>

- **Schuett, J.**, Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). _Towards best practices in AGI safety and governance: A survey of expert opinion_. arXiv. <https://arxiv.org/abs/2305.07153>

- **Schuett, J.** (2023). Defining the scope of AI regulations. _Law, Innovation and Technology, 15_(1), 60–82. <https://doi.org/10.1080/17579961.2023.2184135>

- Mökander, J., **Schuett, J.**, Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. _AI and Ethics_. <https://doi.org/10.1007/s43681-023-00289-2>

- **Schuett, J.** (2023). Risk management in the Artificial Intelligence Act. _European Journal of Risk Regulation_, 1–19. <https://doi.org/10.1017/err.2023.1>

- Cihon, P., **Schuett, J.**, & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. _Information, 12_(7), 275. <https://doi.org/10.3390/info12070275>

- Cihon, P., Kleinaltenkamp, M. J., **Schuett, J.**, & Baum, S. D. (2021). AI certification: Advancing ethical practice by reducing information asymmetries. _IEEE Transactions on Technology and Society, 2_(4), 200–209. <https://doi.org/10.1109/TTS.2021.3077595>

### Policy briefs
- **Schuett, J.**, Koessler, L., & Anderljung, M. (2024). Response to the RFI related to NIST's assignments under the Executive Order concerning AI. _Centre for the Governance of AI_. <https://www.governance.ai/research-paper/response-to-the-rfi-related-to-nists-assignments-under-the-executive-order-concerning-ai>

- **Schuett, J.**, Anderljung, M., Heim, H., & Seger, E. (2023). National priorities for artificial intelligence: Response to the OSTP request for information. _Centre for the Governance of AI_. <https://www.governance.ai/research-paper/national-priorities-for-artificial-intelligence-ostp-response>

- Smith, E. T., **Schuett, J.**, Anderljung, M., & Heim, L. (2023). Response to the NTIA AI accountability policy request for comment. _Centre for the Governance of AI_. <https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy>

- **Schuett, J.**, & Anderljung, M. (2022). Submission to the NIST AI Risk Management Framework. _Centre for the Governance of AI_. <https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework>

## Contact
Find me on [Twitter](https://twitter.com/jonasschuett) and [LinkedIn](https://www.linkedin.com/in/jonasschuett) or reach out to me at [jonas.schuett@governance.ai](mailto:jonas.schuett@governance.ai).

Find my research on [ORCID](https://orcid.org/0000-0001-7154-5049), [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao), and [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327).
