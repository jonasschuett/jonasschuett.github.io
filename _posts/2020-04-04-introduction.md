---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg "Jonas Schuett")

## About me

I'm a Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/). I try to reduce catastrophic risks from AI by improving the governance of frontier AI developers. My research focuses on risk management, corporate governance, regulation, and auditing.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s Public Policy team. I also helped found the [Institute for Law and AI (LawAI)](https://law-ai.org/) where I'm still a board member.

I'm currently wrapping up my PhD in law at Goethe University Frankfurt. I hold a law degree from Heidelberg University and have studied economics at the University of Zurich.

---

## Ongoing research projects

- **How should regulators determine whether a frontier AI model poses unacceptable risks?** (with [Markus Anderljung](https://www.linkedin.com/in/markus-anderljung-21369974/), [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). Regulators could require developers to follow certain rules that are intended to reduce risks to an acceptable level (rules-based approach), or they could require them to achieve certain risk-related outcomes, without specifying how to do that (outcomes-based approach). Since the two approaches have complementary strengths and weaknesses, we argue that regulators should combine them.
 
- **How to estimate the impact and likelihood of risks from AI** (with [Caroline Baumoehl](https://www.linkedin.com/in/caroline-baumoehl-b9030514b/), [Malcolm Murray](https://www.linkedin.com/in/malcolmmurray/), [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). We make the case for explicit risk estimates, list options, and make recommendations.

- **Defining tolerance levels for risks from AI** (with [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). We propose a framework that decision-makers can use to define tolerance levels for risk estimates.

- **AI risk assessment Delphi study** (with [Malcolm Murray](https://www.linkedin.com/in/malcolmmurray/), [Noemi Dreksler](https://www.linkedin.com/in/noemidreksler/), [Markus Anderljung](https://www.linkedin.com/in/markus-anderljung-21369974/), [Ben Garfinkel](https://www.governance.ai/team/ben-garfinkel)). We ask leading domain experts to estimate the impact and likelihood of different risk scenarios using a modified Delphi approach.

- **Passing judgment on responsible scaling policies** (with [Jide Alaga](https://www.governance.ai/team/jide-alaga)). We identify key properties of 
good RSPs, and propose a grading rubric that can be used to evaluate existing ones.

- **How to assess the effectiveness of safeguards** (with [Patrick Levermore](https://www.linkedin.com/in/patrick-levermore-385395b3/)). We list existing safeguards, and discuss different ways of gathering evidence about their effectiveness.

- **Safety cases for frontier AI** (with Marie Buhl, [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)) We make the case for safety cases, recommend key components, and discuss their relevance for self-governance and regulation.

- **AI board governance: Best practices in risk oversight** (with [John Bridge](https://www.linkedin.com/in/johnmichaelbridge/), [Nikhil Mulani](https://www.linkedin.com/in/nmulani/)). First, we give an overview of the board’s responsibility for risk oversight. We explain what fiduciary duties are and how the Delaware Court of Chancery has interpreted these duties in the so-called _Caremark_ cases. Next, we discuss five practices in risk oversight that are implied by _Caremark_ cases. For each of them, we consider instances where the court found that the defendants fell below the _Caremark_ oversight standard, and draw parallels to frontier AI developers. Finally, we consider additional practices that boards might wish to implement, even though they are not directly implied by _Caremark_ cases.

---

## Publications

- **Schuett, J.**, Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. _AI and Ethics_. <https://doi.org/10.1007/s43681-023-00409-y>

- Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. (2023). _Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework_. arXiv. <https://arxiv.org/abs/2311.14711>

- **Schuett, J.** (2023). Three lines of defense against risks from AI. _AI & Society_. <https://doi.org/10.1007/s00146-023-01811-0>

- Alaga, J., & **Schuett, J.** (2023). _Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers_. arXiv. <https://arxiv.org/abs/2310.00374>

- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., **Schuett, J.**, Wei, K., Winter, C., Arnold, M., ÓhÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., Levermore, P., Hazell, J., & Gupta, A. (2023). _Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives_. arXiv. <https://arxiv.org/abs/2311.09227>

- Koessler, L., & **Schuett, J.** (2023). _Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries_. arXiv. <https://arxiv.org/abs/2307.08823>

- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, F., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., **Schuett, J.**, Shavit, Y., Siddarth, D., Trager, R., & Wolf, K. (2023). _Frontier AI regulation: Managing emerging risks to public safety_. arXiv. <https://arxiv.org/abs/2307.03718>

- **Schuett, J.** (2023). _AGI labs need an internal audit function_. arXiv. <https://arxiv.org/abs/2305.17038>

- **Schuett, J.**, Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). _Towards best practices in AGI safety and governance: A survey of expert opinion_. arXiv. <https://arxiv.org/abs/2305.07153>

- **Schuett, J.** (2023). Defining the scope of AI regulations. _Law, Innovation and Technology, 15_(1), 60–82. <https://doi.org/10.1080/17579961.2023.2184135>

- Mökander, J., **Schuett, J.**, Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. _AI and Ethics_. <https://doi.org/10.1007/s43681-023-00289-2>

- **Schuett, J.** (2023). Risk management in the Artificial Intelligence Act. _European Journal of Risk Regulation_, 1–19. <https://doi.org/10.1017/err.2023.1>

- Cihon, P., **Schuett, J.**, & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. _Information, 12_(7), 275. <https://doi.org/10.3390/info12070275>

- Cihon, P., Kleinaltenkamp, M. J., **Schuett, J.**, & Baum, S. D. (2021). AI certification: Advancing ethical practice by reducing information asymmetries. _IEEE Transactions on Technology and Society, 2_(4), 200–209. <https://doi.org/10.1109/TTS.2021.3077595>

- Winter, C., **Schuett, J.**, Martínez, E., Van Arsdale, S., Araújo, R., Hollman, N., Sebo, J., Stawasz, A., O’Keefe, C., & Rotola, G. (2021). Legal priorities research: A research agenda. _Legal Priorities Project_. <https://www.legalpriorities.org/research_agenda.pdf>

---

### Contact

Find me on [Twitter](https://twitter.com/jonasschuett) and [LinkedIn](https://www.linkedin.com/in/jonasschuett) or reach out to me at [jonas.schuett@governance.ai](mailto:jonas.schuett@governance.ai).

Find my research on [ORCID](https://orcid.org/0000-0001-7154-5049), [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao), and [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327).
