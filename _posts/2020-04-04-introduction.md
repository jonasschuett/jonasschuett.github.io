---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg)

## About

<p class="lead">I’m an AI governance researcher and policy advisor. I help governments and AI companies to manage emerging risks from AI.</p>

I'm a Senior Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/) in Oxford, where I lead the risk management workstream.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s Public Policy Team. I also helped found the [Institute for Law and AI (LawAI)](https://law-ai.org/), where I'm still a Non-Executive Board Member.

I’m currently wrapping up my PhD in law at Goethe Frankfurt. I hold a law degree from Heidelberg University and have studied economics at the University of Zurich.

<i class="fa-brands fa-twitter" style="color: #228be6;"></i> [Twitter](https://twitter.com/jonasschuett) &nbsp; <i class="fa-brands fa-linkedin" style="color: #228be6;"></i> [LinkedIn](https://www.linkedin.com/in/jonasschuett) &nbsp; <i class="fa-solid fa-envelope" style="color: #228be6;"></i> [Email](mailto:jonas.schuett@governance.ai)

---

## Research

My research is aimed at reducing societal risks from AI. I’m particularly concerned about emerging risks from frontier AI systems.

With my research, I try to inform high-stakes decisions by policymakers (e.g. the UK government) and frontier AI developers (e.g. OpenAI, Google DeepMind, and Anthropic).

I currently focus on:

- **Risk management.** It becomes increasingly clear that the development and deployment of frontier AI models poses severe risks to society. I study how these risks can be measured and reduced to an acceptable level.

- **Regulation.** Although self-governance plays an important role in reducing emerging risks from AI, we will eventually need frontier AI regulation. I‘m working on several research projects intended to inform the design of a regulatory regime for frontier AI models, especially in the UK and US.

- **Corporate governance.** As frontier AI developers transition from startups to more mature companies, they need to improve their corporate governance. I‘m particularly interested in ways to improve their risk governance.

- **Auditing.** The development and deployment of frontier AI models has significant effects on society. Key governance decisions should therefore not be left to the developers; society needs to have a say. At the minimum, society needs to be informed. A common way to gather information about a model and the governance of its developers is auditing.

---

## Ongoing research projects

**From principles to rules: A regulatory approach for frontier AI** <br>
Jonas Schuett, Markus Anderljung, Leonie Koessler, Alexis Carlier, Ben Garfinkel

**How to estimate the impact and likelihood of risks from AI** <br>
Jonas Schuett, Caroline Baumoehl, Malcolm Murray, Leonie Koessler

**Risk thresholds for frontier AI** <br>
Leonie Koessler, Jonas Schuett, Markus Anderljung

**Estimating risks from LLM: A Delphi study** <br>
Malcolm Murray, Noemi Dreksler, Jonas Schuett, Ben Garfinkel, Markus Anderljung

**Criteria for evaluating frontier AI safety policies** <br>
Jide Alaga, Jonas Schuett

**Safety cases for frontier AI** <br>
Marie Buhl, Gaurav Sett, Leonie Koessler, Jonas Schuett

**How to assess the effectiveness of safeguards** <br>
Patrick Levermore, Jonas Schuett

---

## Publications

**2024**

**How to design an AI ethics board** <br>
Jonas Schuett, Anka Reuel, Alexis Carlier <br>
*AI and Ethics* <br>
[[DOI](https://doi.org/10.1007/s43681-023-00409-y)]

**2023**

**Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework** <br>
Markus Anderljung, Everett T. Smith, Joe O'Brien, Lisa Soder, Benjamin Bucknall, Emma Bluemke, Jonas Schuett, Robert Trager, Lacey Strahm, Rumman Chowdhury <br>
*arXiv preprint arXiv:2311.14711* <br>
[[arXiv](https://arxiv.org/abs/2311.14711)]

**Three lines of defense against risks from AI** <br>
Jonas Schuett <br>
*AI & Society* <br>
[[DOI](https://doi.org/10.1007/s00146-023-01811-0)]

**Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers** <br>
Jide Alaga, Jonas Schuett <br>
*arXiv preprint arXiv:2310.00374* <br>
[[arXiv](https://arxiv.org/abs/2310.00374)]

**Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives** <br>
Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek, Markus Anderljung, Ben Bucknall, Alan Chan, Eoghan Stafford, Leonie Koessler, Aviv Ovadya, Ben Garfinkel, Emma Bluemke, Michael Aird, Patrick Levermore, Julian Hazell, Abhishek Gupta <br>
*arXiv preprint arXiv:2311.09227* <br>
[[arXiv](https://arxiv.org/abs/2311.09227)]

**Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries** <br>
Leonie Koessler, Jonas Schuett <br>
*arXiv preprint arXiv:2307.08823* <br>
[[arXiv](https://arxiv.org/abs/2307.08823)]

**Frontier AI regulation: Managing emerging risks to public safety** <br>
Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O'Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf <br>
*arXiv preprint arXiv:2307.03718* <br>
[[arXiv](https://arxiv.org/abs/2307.03718)]

**AGI labs need an internal audit function** <br>
Jonas Schuett <br>
*arXiv preprint arXiv:2305.17038* <br>
[[arXiv](https://arxiv.org/abs/2305.17038)]

**Towards best practices in AGI safety and governance: A survey of expert opinion** <br>
Jonas Schuett, Noemi Dreksler, Markus Anderljung, David McCaffary, Lennart Heim, Emma Bluemke, Ben Garfinkel <br>
*arXiv preprint arXiv:2305.07153* <br>
[[arXiv](https://arxiv.org/abs/2305.07153)]

**Defining the scope of AI regulations** <br>
Jonas Schuett <br>
*Law, Innovation and Technology, 15*(1), 60–82 <br>
[[DOI](https://doi.org/10.1080/17579961.2023.2184135)]

**Auditing large language models: A three-layered approach** <br>
Jakob Mökander, Jonas Schuett, Hannah R. Kirk, Luciano Floridi <br>
*AI and Ethics* <br>
[[DOI](https://doi.org/10.1007/s43681-023-00289-2)]

**Risk management in the Artificial Intelligence Act** <br>
Jonas Schuett <br>
*European Journal of Risk Regulation*, 1–19 <br>
[[DOI](https://doi.org/10.1017/err.2023.1)]

**2021**

**AI certification: Advancing ethical practice by reducing information asymmetries** <br>
Peter Cihon, Moritz J. Kleinaltenkamp, Jonas Schuett, Seth D. Baum <br>
*IEEE Transactions on Technology and Society, 2*(4), 200–209 <br>
[[DOI](https://doi.org/10.1109/TTS.2021.3077595)]

**Corporate governance of artificial intelligence in the public interest** <br>
Peter Cihon, Jonas Schuett, Seth D. Baum <br>
*Information, 12*(7), 275 <br>
[[DOI](https://doi.org/10.3390/info12070275)]

---

## Submissions

**2024**

**Comments on NIST’s Draft Profile on Generative AI** <br>
Malcolm Murray, Jonas Schuett, Sam Manning, Alan Chan, Leonie Koessler, Markus Anderljung
*Centre for the Governance of AI* <br>
[[PDF](https://cdn.governance.ai/Comments_on_NISTs_Draft_Profile_on_Generative_AI.pdf)] [[RFI](https://airc.nist.gov/docs/NIST.AI.600-1.GenAI-Profile.ipd.pdf)]

**Response to the RFI related to NIST's assignments under the Executive Order concerning AI** <br>
Jonas Schuett, Leonie Koessler, Markus Anderljung <br>
*Centre for the Governance of AI* <br>
[[PDF](https://cdn.governance.ai/GovAI_Response_to_RFI_Related_to_NIST_Assignments_Under_Executive_Order_Concerning_AI.pdf)] [[RFI](https://www.federalregister.gov/documents/2023/12/21/2023-28232/request-for-information-rfi-related-to-nists-assignments-under-sections-41-45-and-11-of-the)]

**2023**

**National priorities for AI: Response to the OSTP request for information** <br>
Jonas Schuett, Markus Anderljung, Lennart Heim, Elizabeth Seger <br>
*Centre for the Governance of AI* <br>
[[PDF](https://cdn.governance.ai/Response_to_the_OSTP_Request_for_Information__National_Priorities_for_Artificial_Intelligence.pdf)] [[RFI](https://www.whitehouse.gov/wp-content/uploads/2023/05/OSTP-Request-for-Information-National-Priorities-for-Artificial-Intelligence.pdf)]

**Response to the NTIA AI accountability policy request for comment** <br>
Everett T. Smith, Jonas Schuett, Markus Anderljung, Lennart Heim <br>
*Centre for the Governance of AI* <br>
[[PDF](https://cdn.governance.ai/GovAI_Response_to_the_NTIA_AI_Accountability_Policy_Request_for_Comment.pdf)] [[RFI](https://www.federalregister.gov/documents/2023/04/13/2023-07776/ai-accountability-policy-request-for-comment)]

**2022**

**Submission to the NIST AI Risk Management Framework** <br>
Jonas Schuett, Markus Anderljung <br>
*Centre for the Governance of AI* <br>
[[PDF](https://cdn.governance.ai/Comments_on_the_Initial_Draft_of_the_NIST_AI_RMF_-_GovAI.pdf)] [[RFI](https://www.nist.gov/system/files/documents/2022/03/17/AI-RMF-1stdraft.pdf)]

---

[Twitter](https://twitter.com/jonasschuett) • [LinkedIn](https://www.linkedin.com/in/jonasschuett) • [Email](mailto:jonas.schuett@governance.ai) • [ORCID](https://orcid.org/0000-0001-7154-5049) • [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao) • [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327)
