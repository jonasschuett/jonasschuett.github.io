---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg "Jonas Schuett")

## About me

I'm a Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/). I try to reduce catastrophic risks from AI by improving the governance of frontier AI developers. My research focuses on risk management, corporate governance, regulation, and auditing.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s Public Policy team. I also helped found the [Institute for Law and AI (LawAI](https://law-ai.org/) where I'm still a board member.

I'm currently wrapping up my PhD in law at Goethe University Frankfurt. I hold a law degree from Heidelberg University and have studied economics at the University of Zurich.

## Ongoing research projects

- **How should regulators determine whether a frontier AI model poses unacceptable risks?** (w/ [Markus Anderljung](https://www.linkedin.com/in/markus-anderljung-21369974/), [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). Regulators could require developers to follow certain rules that are intended to reduce risks to an acceptable level (rules-based approach), or they could require them to achieve certain risk-related outcomes, without specifying how to do that (outcomes-based approach). Since the two approaches have complementary strengths and weaknesses, we argue that regulators should combine them.
 
- **How to estimate the impact and likelihood of risks from AI** (w/ [Caroline Baumoehl](https://www.linkedin.com/in/caroline-baumoehl-b9030514b/), [Malcolm Murray](https://www.linkedin.com/in/malcolmmurray/), [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). We make the case for explicit risk estimates, list options, and make recommendations.

- **Defining tolerance levels for risks from AI** (w/ [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)). We propose a framework that decision-makers can use to define tolerance levels for risk estimates.

- **AI risk assessment Delphi study** (w/ [Malcolm Murray](https://www.linkedin.com/in/malcolmmurray/), Noemi Dreksler, [Markus Anderljung](https://www.linkedin.com/in/markus-anderljung-21369974/), Ben Garfinkel). We ask leading domain experts to estimate the impact and likelihood of different risk scenarios using a modified Delphi approach.

- **Passing judgment on responsible scaling policies** (w/ [Jide Alaga](https://www.governance.ai/team/jide-alaga)). We identify key properties of 
good RSPs, and propose a grading rubric that can be used to evaluate existing ones.

- **How to assess the effectiveness of safeguards** (w/ [Patrick Levermore](https://www.linkedin.com/in/patrick-levermore-385395b3/)). We list existing safeguards, and discuss different ways of gathering evidence about their effectiveness.

- **Safety cases for frontier AI** (w/ Marie Buhl, [Leonie Koessler](https://www.linkedin.com/in/leonie-koessler-ll-m-kcl-85b71814a/)) We make the case for safety cases, recommend key components, and discuss their relevance for self-governance and regulation.

- **AI board governance: Best practices in risk oversight** (w/ [John Bridge](https://www.linkedin.com/in/johnmichaelbridge/), [Nikhil Mulani](https://www.linkedin.com/in/nmulani/)). […]


## Risk management

I do a lot of work on AI risk management. I try to figure out how frontier AI developers should manage catastrophic risks from AI.

I'm currently working on several projects that focus on various aspects of risk assessment: we're proposing a taxonomy of catastrophic risks from AI, developing a quantitative risk model, and reviewing risk thresholds from other safety critical industries. Besides that, I have recently published a paper in which we [review popular risk assessment techniques](https://arxiv.org/abs/2307.08823) from other industries.

In the past, I have also worked on several projects related to risk governance. I have one paper on the [Three Lines of Defense model](https://arxiv.org/abs/2212.08364) and anohter one on [internal audit](https://arxiv.org/abs/2305.17038). I also have one paper in which I conduct a [legal analysis of the main risk management provision in the proposed EU AI Act](https://doi.org/10.1017/err.2023.1).

![Risk Management](/risk_management.jpg "Risk Management")


## Corporate governance

My second focus is on corporate governance. Earlier this year, we published a [survey of AGI safety and governance experts](https://arxiv.org/abs/2305.07153). We also published a paper in which we make recommendations on [how to design an AI ethics board](https://arxiv.org/abs/2304.07249). A while back, I also contributed to an [introductory paper on corporate governance](https://doi.org/10.3390/info12070275).


## Auditing

My third focus is on auditing. I just published a paper in which we propose an [evaluation-based coordination scheme](https://arxiv.org/abs/2310.00374). I also contributed to a paper on [auditing large language models](https://doi.org/10.1007/s43681-023-00289-2) and [AI certification](https://doi.org/10.1109/TTS.2021.3077595).


## Regulation

Finally, I've done some work on AI regulation. On the theoretical side of things, I contributed to a paper on [frontier AI regulation](https://arxiv.org/abs/2307.03718). I also have a paper on [defining the scope of AI regulations](https://doi.org/10.1080/17579961.2023.2184135) and, as mentioned above, a paper on [risk management in the AI Act](https://doi.org/10.1017/err.2023.1).

On the practical side of things, I was seconded to the UK Cabinet Office to support their work on AI regulation. I mainly gave input on the white paper [A pro-innovation approach to AI regulation](https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper).

---

## Work experience

- **Centre for the Governance of AI (GovAI)** • Research Fellow <br>
2022–present

- **UK Cabinet Office** • Expert Advisor (Artificial Intelligence) <br>
2022

- **Google DeepMind** • Policy Research and Intelligence Intern <br>
2021

- **Institute for Law and AI (LawAI)** • Research Fellow <br>
2020–2022

- **KPMG Law** • Consultant <br>
2018–2020

---

## Education

- **Goethe University Frankfurt** • PhD, Law <br>
2018–present

- **Heidelberg University** • MJur / First State Exam, Law <br>
2012–2018

- **University of Zurich** • Economics <br>
2011–2012

---

### Publications

- **Schuett, J.**, Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. _AI and Ethics_. [https://doi.org/10.1007/s43681-023-00409-y](https://doi.org/10.1007/s43681-023-00409-y)

- Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. (2023). _Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework_. arXiv. [https://arxiv.org/abs/2311.14711](https://arxiv.org/abs/2311.14711)

- **Schuett, J.** (2023). Three lines of defense against risks from AI. _AI & Society_. [https://doi.org/10.1007/s00146-023-01811-0](https://doi.org/10.1007/s00146-023-01811-0)

- Alaga, J., & **Schuett, J.** (2023). _Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers_. arXiv. [https://arxiv.org/abs/2310.00374](https://arxiv.org/abs/2310.00374)

- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., **Schuett, J.**, Wei, K., Winter, C., Arnold, M., ÓhÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., Levermore, P., Hazell, J., & Gupta, A. (2023). _Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives_. arXiv. [https://arxiv.org/abs/2311.09227](https://arxiv.org/abs/2311.09227)

- Koessler, L., & **Schuett, J.** (2023). _Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries_. arXiv. [https://arxiv.org/abs/2307.08823](https://arxiv.org/abs/2307.08823)

- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, F., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., **Schuett, J.**, Shavit, Y., Siddarth, D., Trager, R., & Wolf, K. (2023). _Frontier AI regulation: Managing emerging risks to public safety_. arXiv. [https://arxiv.org/abs/2307.03718](https://arxiv.org/abs/2307.03718)

- **Schuett, J.** (2023). _AGI labs need an internal audit function_. arXiv. [https://arxiv.org/abs/2305.17038](https://arxiv.org/abs/2305.17038)

- **Schuett, J.**, Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). _Towards best practices in AGI safety and governance: A survey of expert opinion_. arXiv. [https://arxiv.org/abs/2305.07153](https://arxiv.org/abs/2305.07153)

- **Schuett, J.** (2023). Defining the scope of AI regulations. _Law, Innovation and Technology, 15_(1), 60–82. [https://doi.org/10.1080/17579961.2023.2184135](https://doi.org/10.1080/17579961.2023.2184135)

- Mökander, J., **Schuett, J.**, Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. _AI and Ethics_. [https://doi.org/10.1007/s43681-023-00289-2](https://doi.org/10.1007/s43681-023-00289-2)

- **Schuett, J.** (2023). Risk management in the Artificial Intelligence Act. _European Journal of Risk Regulation_, 1–19. [https://doi.org/10.1017/err.2023.1](https://doi.org/10.1017/err.2023.1)

- Cihon, P., **Schuett, J.**, & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. _Information, 12_(7), 275. [https://doi.org/10.3390/info12070275](https://doi.org/10.3390/info12070275)

- Cihon, P., Kleinaltenkamp, M. J., **Schuett, J.**, & Baum, S. D. (2021). AI certification: Advancing ethical practice by reducing information asymmetries. _IEEE Transactions on Technology and Society, 2_(4), 200–209. [https://doi.org/10.1109/TTS.2021.3077595](https://doi.org/10.1109/TTS.2021.3077595)

- Winter, C., **Schuett, J.**, Martínez, E., Van Arsdale, S., Araújo, R., Hollman, N., Sebo, J., Stawasz, A., O’Keefe, C., & Rotola, G. (2021). Legal priorities research: A research agenda. _Legal Priorities Project_. [https://www.legalpriorities.org/research_agenda.pdf](https://www.legalpriorities.org/research_agenda.pdf)

---

### Personal

I live in Berlin, but come to Oxford every 1-2 months. I’m engaged to my high school sweetheart, and we have the best dog in the world. I’m a big fan of specialty coffee---I've been called a "coffee snob". I’m vegan to avoid unnecessary suffering of animals.

---

### Contact

Find me on [Twitter](https://twitter.com/jonasschuett) and [LinkedIn](https://www.linkedin.com/in/jonasschuett) or reach out to me at [jonas.schuett@governance.ai](mailto:jonas.schuett@governance.ai).

Find my research on [ORCID](https://orcid.org/0000-0001-7154-5049), [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao), and [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327).
