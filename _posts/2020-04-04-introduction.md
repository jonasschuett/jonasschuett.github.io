---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg)

## About

I'm a Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/) in Oxford. Within the policy team, I lead the workstream on risk management and corporate governance.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s public policy team. I also helped found the [Institute for Law and AI (LawAI)](https://law-ai.org/), where I'm still a board member.

I’m currently wrapping up my PhD in law at Goethe Frankfurt. I hold a law degree from Heidelberg University, and have studied economics at the University of Zurich.

I live in Berlin, but come to Oxford every 4-6 weeks.

<i class="fa-brands fa-twitter" style="color: #228be6;"></i> [Twitter](https://twitter.com/jonasschuett) &nbsp; <i class="fa-brands fa-linkedin" style="color: #228be6;"></i> [LinkedIn](https://www.linkedin.com/in/jonasschuett) &nbsp; <i class="fa-solid fa-envelope" style="color: #228be6;"></i> [Email](mailto:jonas.schuett@governance.ai)

---

## Research

My research is aimed at reducing societal risks from AI. I’m particularly concerned about emerging risks from frontier AI models.

I try to inform high-stakes governance decisions by policymakers (e.g. the UK government) and frontier AI developers (e.g. OpenAI, Google DeepMind, and Anthropic).

[Risk management](#risk-management) • [Regulation](#regulation) • [Corporate governance](#corporate-governance) • [Auditing](#auditing)

---

## Risk management
It becomes increasingly clear that the development and deployment of frontier AI models poses severe risks to society. I study how these risks can be measured and reduced to an acceptable level.
{: .message }

Ongoing research projects:

- **AI risk assessment Delphi study** (with Malcolm Murray, Noemi Dreksler, Markus Anderljung, and Ben Garfinkel). We ask leading domain experts to estimate the impact and likelihood of different risk scenarios using a modified Delphi approach.

- **How to estimate the impact and likelihood of risks from AI** (with Caroline Baumoehl, Malcolm Murray, and Leonie Koessler). We make the case for explicit risk estimates, list options, and make recommendations.

- **Defining tolerance levels for risks from AI** (with Leonie Koessler). We propose a framework that decision-makers can use to define tolerance levels for risk estimates.

- **How to assess the effectiveness of safeguards** (with Patrick Levermore). We list existing safeguards, and discuss different ways of gathering evidence about their effectiveness.

- **Passing judgment on responsible scaling policies** (with Jide Alaga). We identify key properties of  good RSPs, and propose a grading rubric that can be used to evaluate existing ones.

Previous research projects:

- **Risk management in the AI Act.** The EU AI Act is the most comprehensive attempt to regulate AI in a major jurisdiction. In this article, I conduct a legal analysis of Art. 9, the key risk management provision. I give an overview of the regulatory concept behind the norm, determine its purpose and scope of application, offer a comprehensive interpretation of the specific risk management requirements, and outline ways in which the requirements can be enforced. [Read](https://doi.org/10.1017/err.2023.1) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Risk assessments at AGI companies** (with Leonie Koessler). We review popular risk assessment techniques from other industries and discuss how companies that have the stated goal of build AGI could use them. [Read](https://arxiv.org/abs/2307.08823) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers** (with Jide Alaga). Frontier AI developers increasingly conduct model evaluations to identify potentially  dangerous capabilities. But what should they do if sufficiently dangerous capabilities are in fact discovered? In this paper, we propose an evaluation-based coordination scheme. We also discuss the desirability and feasibility of that scheme. [Read](https://arxiv.org/abs/2307.08823) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

![Risk management](/risk_management.png)

---

## Regulation

Although self-governance plays an important role in reducing emerging risks from AI, we will eventually need frontier AI regulation. I‘m working on several research projects intended to inform the design of a regulatory regime for frontier AI models, especially in the UK and US.
{: .message }

Ongoing research projects:

- **How should regulators determine whether a frontier AI model poses unacceptable risks?** (with Markus Anderljung, Leonie Koessler, and Ben Garfinkel). Regulators could require developers to follow certain rules that are intended to reduce risks to an acceptable level (rules-based approach), or they could require them to achieve certain risk-related outcomes, without specifying how to do that (outcomes-based approach). Since the two approaches have complementary strengths and weaknesses, we argue that regulators should combine them.

- **Safety cases for frontier AI** (with Marie Buhl, Leonie Koessler, and Markus Anderljung). We make the case for safety cases, recommend key components, and discuss their relevance for self-governance and regulation.

Previous research projects:

- **Frontier AI regulation: Managing emerging risks to public safety** (with Markus Anderljung and others). This multi-author report is the first attempt to sketch a regulatory regime for frontier AI models. [Read](https://arxiv.org/abs/2307.03718) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Defining the scope of AI regulations.** In this paper, I argue that the material scope of AI regulations should not rely on the term AI, mainly because existing AI definitions don’t meet the most important requirements for legal definitions. Rather than using the term AI, policymakers should focus on the specific risks they want to reduce. I show that that the requirements for legal definitions can be better met by defining the main sources of relevant risks: certain technical approaches, applications, and capabilities. [Read](https://doi.org/10.1080/17579961.2023.2184135) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

![Regulation](/regulation.png)

---

## Corporate governance

As frontier AI developers transition from startups to more mature companies, they need to improve their corporate governance. I‘m particularly interested in ways to improve their risk governance.
{: .message }

Ongoing research projects:

- **AI board governance: Best practices in risk oversight** (with John Bridge and Nikhil Mulani). In this paper, we give an overview of the board’s responsibility for risk oversight. We explain what fiduciary duties are and how the Delaware Court of Chancery has interpreted these duties in the so-called _Caremark_ cases. Next, we discuss five practices in risk oversight that are implied by _Caremark_ cases. We also consider additional practices that boards may want to implement, even though they are not directly implied by _Caremark_ cases.

Previous research projects:

- **Towards best practices in AGI safety and governance: A survey of expert opinion** (with Noemi Dreksler and others). In this survey, we asked leading experts from AGI labs, academia, and civil society how much they agree with 50 statements about different AGI safety and governance practices. We found surprising support for all of them. [Read](https://arxiv.org/abs/2305.07153) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **How to design an AI ethics board** (with Anka Reuel and Alexis Carlier). Several failures have shown that designing an AI ethics board can be challenging. In this paper, we provide a toolbox that AI companies can use to overcome these challenges. We identify key design choices and discuss how each of them affects the board‘s ability to reduce societal risks from AI. [Read](https://doi.org/10.1007/s43681-023-00409-y) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Three lines of defense against risks from AI.** The Three Lines of Defense model is a popular risk governance framework. It helps organizations to assign and coordinate different risk management roles and responsibilities. In this paper, I apply the model to an AI context. [Read](https://doi.org/10.1007/s00146-023-01811-0) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Frontier AI developers need an internal audit function.** The internal audit function evaluates the adequacy and effectiveness of a company’s risk management, control, and governance processes. It is organizationally independent from senior management and reports directly to the board of directors. In this paper, I argue that frontier AI developers need an internal audit function. [Read](https://arxiv.org/abs/2305.17038) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Corporate governance of AI in the public interest** (with Peter Cihon and Seth Baum). In this paper, we discuss how different actors can influence corporate decision-making in a way that advances the public interest. We consider actors inside the corporation (managers, workers, and investors) and outside the corporation (corporate partners and competitors, industry consortia, nonprofit organizations, the public, the media, and governments). [Read](https://doi.org/10.3390/info12070275) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

![Corporate governance](/corporate_governance.png)

---

## Auditing

The development and deployment of frontier AI models has significant effects on society. Key governance decisions should therefore not be left to the developers; society needs to have a say. At the minimum, society needs to be informed. A common way to gather information about a model and the governance of its developers is auditing.
{: .message }

Previous research projects:

- **Auditing large language models: A three-layered approach** (with Jakob Mökander, Hannah Kirk, and Luciano Floridi). In this paper, we argue that existing auditing methodologies are insufficient to address the specific risks posed by large language models (LLMs). We propose a new auditing methodology for LLMs, in which governance audits, model audits, and application audits complement and inform each other. [Read](https://doi.org/10.1007/s43681-023-00289-2) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework** (with Markus Anderljung and others). In this paper, we survey six requirements for effective external scrutiny of frontier AI systems and organize them under the ASPIRE framework: Access, searching attitude, proportionality to the risks, independence, resources, and expertise. [Read](https://arxiv.org/abs/2311.14711) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

- **AI certification: Advancing ethical practice by reducing information asymmetries** (with Peter Cihon, Moritz Kleinaltenkamp, and Seth Baum). In this paper, we examine to what extent certification can incentivize adoption of AI ethics principles and substantiate that they have been implemented in practice. We draw from management literature on certification and review current AI certification programs and proposals. [Read](https://doi.org/10.1109/TTS.2021.3077595) <i class="fa-solid fa-circle-arrow-right" style="color: #228be6;"></i>

![Auditing](/auditing.png)

---

## Publications

- **Schuett, J.**, Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. *AI and Ethics*. <https://doi.org/10.1007/s43681-023-00409-y>

- Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. (2023). *Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework*. arXiv. <https://arxiv.org/abs/2311.14711>

- **Schuett, J.** (2023). Three lines of defense against risks from AI. *AI & Society*. <https://doi.org/10.1007/s00146-023-01811-0>

- Alaga, J., & **Schuett, J.** (2023). *Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers*. arXiv. <https://arxiv.org/abs/2310.00374>

- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., **Schuett, J.**, Wei, K., Winter, C., Arnold, M., ÓhÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., Levermore, P., Hazell, J., & Gupta, A. (2023). *Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives*. arXiv. <https://arxiv.org/abs/2311.09227>

- Koessler, L., & **Schuett, J.** (2023). *Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries*. arXiv. <https://arxiv.org/abs/2307.08823>

- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, F., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., **Schuett, J.**, Shavit, Y., Siddarth, D., Trager, R., & Wolf, K. (2023). *Frontier AI regulation: Managing emerging risks to public safety*. arXiv. <https://arxiv.org/abs/2307.03718>

- **Schuett, J.** (2023). _AGI labs need an internal audit function_. arXiv. <https://arxiv.org/abs/2305.17038>

- **Schuett, J.**, Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). *Towards best practices in AGI safety and governance: A survey of expert opinion*. arXiv. <https://arxiv.org/abs/2305.07153>

- **Schuett, J.** (2023). Defining the scope of AI regulations. *Law, Innovation and Technology, 15*(1), 60–82. [https://doi.org/10.1080/17579961-2023-2184135](https://doi.org/10.1080/17579961.2023.2184135)

- Mökander, J., **Schuett, J.**, Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. *AI and Ethics*. <https://doi.org/10.1007/s43681-023-00289-2>

- **Schuett, J.** (2023). Risk management in the Artificial Intelligence Act. *European Journal of Risk Regulation*, 1–19. <https://doi.org/10.1017/err.2023.1>

- Cihon, P., **Schuett, J.**, & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. *Information, 12*(7), 275. <https://doi.org/10.3390/info12070275>

- Cihon, P., Kleinaltenkamp, M. J., **Schuett, J.**, & Baum, S. D. (2021). AI certification: Advancing ethical practice by reducing information asymmetries. *IEEE Transactions on Technology and Society, 2*(4), 200–209. <https://doi.org/10.1109/TTS.2021.3077595>

---

## Publications 2

**2024**

**How to design an AI ethics board** <br>
**Jonas Schuett**, Anka Reuel, & Alexis Carlier <br>
*AI and Ethics* <br>
[[DOI](https://doi.org/10.1007/s43681-023-00409-y)]

**2023**

**Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework** <br>
Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. <br>
*arXiv:2311.14711* <br>
[[arXiv](https://arxiv.org/abs/2311.14711)]

**2021**

**AI certification: Advancing ethical practice by reducing information asymmetries** <br>
Peter Cihon, Moritz J. Kleinaltenkamp, **Jonas Schuett**, & Seth D. Baum <br>
*IEEE Transactions on Technology and Society, 2*(4), 200–209 <br>
[[DOI](https://doi.org/10.1109/TTS.2021.3077595)]

**Corporate governance of artificial intelligence in the public interest** <br>
Peter Cihon, **Jonas Schuett**, & Seth D. Baum <br>
*Information, 12*(7), 275 <br>
[[DOI](https://doi.org/10.3390/info12070275)]

---

## Submissions
 
- **Schuett, J.**, Koessler, L., & Anderljung, M. (2024). Response to the RFI related to NIST's assignments under the Executive Order concerning AI. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/response-to-the-rfi-related-to-nists-assignments-under-the-executive-order-concerning-ai>

- **Schuett, J.**, Anderljung, M., Heim, H., & Seger, E. (2023). National priorities for AI: Response to the OSTP request for information. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/national-priorities-for-artificial-intelligence-ostp-response>

- Smith, E. T., **Schuett, J.**, Anderljung, M., & Heim, L. (2023). Response to the NTIA AI accountability policy request for comment. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy>

- **Schuett, J.**, & Anderljung, M. (2022). Submission to the NIST AI Risk Management Framework. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework>

---

[Twitter](https://twitter.com/jonasschuett) • [LinkedIn](https://www.linkedin.com/in/jonasschuett) • [Email](mailto:jonas.schuett@governance.ai) • [ORCID](https://orcid.org/0000-0001-7154-5049) • [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao) • [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327)
