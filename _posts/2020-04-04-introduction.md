---
layout: page
---

![Jonas Schuett](/jonasschuett.jpg "Jonas Schuett")

## About

I'm a Research Fellow at the [Centre for the Governance of AI (GovAI)](https://www.governance.ai/) in Oxford. Within the policy team, I lead the workstream on risk management and corporate governance.

Before joining GovAI, I was seconded to the UK Cabinet Office to support their work on AI regulation, and interned at Google DeepMind’s public policy team. I also helped found the [Institute for Law and AI (LawAI)](https://law-ai.org/), where I'm still a board member.

I’m currently wrapping up my PhD in law at Goethe University Frankfurt. I hold a law degree from Heidelberg University, and have studied economics at the University of Zurich.

I live in Berlin, but come to Oxford every four to six weeks. I‘m also regularly in San Francisco and in Washington D.C.

[Twitter](https://twitter.com/jonasschuett) • [LinkedIn](https://www.linkedin.com/in/jonasschuett) • [Email](mailto:jonas.schuett@governance.ai)

---

## Research

My research is aimed at reducing societal risks from AI. I’m particularly concerned about emerging risks from frontier AI models.

I try to inform high-stakes governance decisions by policymakers (e.g. the US and UK government) and frontier AI developers (e.g. OpenAI, Google DeepMind, and Anthropic).

This includes decisions about risk management practices, regulation, corporate governance, and auditing.

### Risk management
It becomes increasingly clear that the development and deployment of frontier AI models poses severe risks to society. I study how these risks can be measured and reduced to an acceptable level.

Ongoing research projects:

- **AI risk assessment Delphi study** (with Malcolm Murray, Noemi Dreksler, Markus Anderljung, and Ben Garfinkel). We ask leading domain experts to estimate the impact and likelihood of different risk scenarios using a modified Delphi approach.

- **How to estimate the impact and likelihood of risks from AI** (with Caroline Baumoehl, Malcolm Murray, and Leonie Koessler). We make the case for explicit risk estimates, list options, and make recommendations.

- **Defining tolerance levels for risks from AI** (with Leonie Koessler). We propose a framework that decision-makers can use to define tolerance levels for risk estimates.

- **How to assess the effectiveness of safeguards** (with Patrick Levermore). We list existing safeguards, and discuss different ways of gathering evidence about their effectiveness.

- **Passing judgment on responsible scaling policies** (with Jide Alaga). We identify key properties of  good RSPs, and propose a grading rubric that can be used to evaluate existing ones.

Previous research projects:

- **Risk management in the AI Act.** The EU AI Act is the most comprehensive attempt to regulate AI in a major jurisdiction. In this article, I conduct a legal analysis of Art. 9, the key risk management provision. [[Read](https://doi.org/10.1017/err.2023.1)]

- **Risk assessments at AGI companies** (with Leonie Koessler). We review popular risk assessment techniques from other industries and discuss how companies that have the stated goal of build AGI could use them. [[Read](https://arxiv.org/abs/2307.08823)]

![Risk management](/risk_management.png)

### Regulation

Although self-governance plays an important role in reducing emerging risks from AI, we will eventually need frontier AI regulation. I‘m working on several research projects intended to inform the design of a regulatory regime for frontier AI models, especially in the UK and US.

Ongoing research projects:

- **How should regulators determine whether a frontier AI model poses unacceptable risks?** (with Markus Anderljung, Leonie Koessler, and Ben Garfinkel). Regulators could require developers to follow certain rules that are intended to reduce risks to an acceptable level (rules-based approach), or they could require them to achieve certain risk-related outcomes, without specifying how to do that (outcomes-based approach). Since the two approaches have complementary strengths and weaknesses, we argue that regulators should combine them.

- **Safety cases for frontier AI** (with Marie Buhl, Leonie Koessler, and Markus Anderljung). We make the case for safety cases, recommend key components, and discuss their relevance for self-governance and regulation.

Previous research projects:

- **Frontier AI regulation: Managing emerging risks to public safety** (with Markus Anderljung and others). This multi-author report is the first attempt to sketch a regulatory regime for frontier AI models. [[Read](https://arxiv.org/abs/2307.03718)]

- **Defining the scope of AI regulations.** In this article, I argue that the material scope of AI regulation should not rely on the term AI. Instead, policy makers should take a risk-based approach. They should identify the features of AI systems that drive key risks, and then define these features. These features might be related to the system’s design, application, or capabilities. [[Read](https://doi.org/10.1080/17579961.2023.2184135)]

![Regulation](/regulation.png)

### Corporate governance

As frontier AI developers transition from startups to more mature companies, they need to improve their corporate governance. I‘m particularly interested in ways to improve their risk governance.

Ongoing research projects:

- **AI board governance: Best practices in risk oversight** (with John Bridge and Nikhil Mulani). First, we give an overview of the board’s responsibility for risk oversight. We explain what fiduciary duties are and how the Delaware Court of Chancery has interpreted these duties in the so-called _Caremark_ cases. Next, we discuss five practices in risk oversight that are implied by _Caremark_ cases. For each of them, we consider instances where the court found that the defendants fell below the _Caremark_ oversight standard, and draw parallels to frontier AI developers. Finally, we consider additional practices that boards might wish to implement, even though they are not directly implied by _Caremark_ cases.

Previous research projects:

- **How to design an AI ethics board** (with Anka Reuel and Alexis Carlier). We identify key design choices and discuss how each of them affects the board‘s ability to reduce societal risks from AI. [[Read](https://doi.org/10.1007/s43681-023-00409-y)]

- **Three lines of defense against risks from AI.** […] [[Read](https://doi.org/10.1007/s00146-023-01811-0)]

- **Frontier AI developers need an internal audit function.** […] [[Read](https://arxiv.org/abs/2305.17038)]

- **Corporate governance of AI in the public interest** (with Peter Cihon and Seth Baum). […] [[Read](https://doi.org/10.3390/info12070275)]

![Corporate governance](/corporate_governance.png)

### Auditing

The development and deployment of frontier AI models has significant effects on society. Key governance decisions should therefore not be left to the developers; society needs to have a say. At the minimum, society needs to be informed. A common way to gather information about a model and the governance of its developers is auditing.

Previous research projects:

- **Auditing large language models: A three-layered approach** (with Jakob Mökander, Hannah Kirk, and Luciano Floridi). […] [[Read](https://doi.org/10.1007/s43681-023-00289-2)]

- **Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework** (with Markus Anderljung and others). […] [[Read](https://arxiv.org/abs/2311.14711)]

- **AI certification: Advancing ethical practice by reducing information asymmetries** (with Peter Cihon, Moritz Kleinaltenkamp, and Seth Baum). […] [[Read](https://doi.org/10.1109/TTS.2021.3077595)]

![Auditing](/auditing.png)

---

## Publications

- **Schuett, J.**, Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. *AI and Ethics*. <https://doi.org/10.1007/s43681-023-00409-y>

- Anderljung, M., Smith, E. T., O’Brien, J., Soder, L., Bucknall, B., Bluemke, E., **Schuett, J.**, Trager, R., Strahm, L., & Chowdhury, R. (2023). *Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework*. arXiv. <https://arxiv.org/abs/2311.14711>

- **Schuett, J.** (2023). Three lines of defense against risks from AI. *AI & Society*. <https://doi.org/10.1007/s00146-023-01811-0>

- Alaga, J., & **Schuett, J.** (2023). *Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers*. arXiv. <https://arxiv.org/abs/2310.00374>

- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., **Schuett, J.**, Wei, K., Winter, C., Arnold, M., ÓhÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., Levermore, P., Hazell, J., & Gupta, A. (2023). *Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives*. arXiv. <https://arxiv.org/abs/2311.09227>

- Koessler, L., & **Schuett, J.** (2023). *Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries*. arXiv. <https://arxiv.org/abs/2307.08823>

- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, F., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., **Schuett, J.**, Shavit, Y., Siddarth, D., Trager, R., & Wolf, K. (2023). *Frontier AI regulation: Managing emerging risks to public safety*. arXiv. <https://arxiv.org/abs/2307.03718>

- **Schuett, J.** (2023). _AGI labs need an internal audit function_. arXiv. <https://arxiv.org/abs/2305.17038>

- **Schuett, J.**, Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). *Towards best practices in AGI safety and governance: A survey of expert opinion*. arXiv. <https://arxiv.org/abs/2305.07153>

- **Schuett, J.** (2023). Defining the scope of AI regulations. *Law, Innovation and Technology, 15*(1), 60–82. [https://doi.org/10.1080/17579961-2023-2184135](https://doi.org/10.1080/17579961.2023.2184135)

- Mökander, J., **Schuett, J.**, Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. *AI and Ethics*. <https://doi.org/10.1007/s43681-023-00289-2>

- **Schuett, J.** (2023). Risk management in the Artificial Intelligence Act. *European Journal of Risk Regulation*, 1–19. <https://doi.org/10.1017/err.2023.1>

- Cihon, P., **Schuett, J.**, & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. *Information, 12*(7), 275. <https://doi.org/10.3390/info12070275>

- Cihon, P., Kleinaltenkamp, M. J., **Schuett, J.**, & Baum, S. D. (2021). AI certification: Advancing ethical practice by reducing information asymmetries. *IEEE Transactions on Technology and Society, 2*(4), 200–209. <https://doi.org/10.1109/TTS.2021.3077595>

---

## Submissions
 
- **Schuett, J.**, Koessler, L., & Anderljung, M. (2024). Response to the RFI related to NIST's assignments under the Executive Order concerning AI. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/response-to-the-rfi-related-to-nists-assignments-under-the-executive-order-concerning-ai>

- **Schuett, J.**, Anderljung, M., Heim, H., & Seger, E. (2023). National priorities for AI: Response to the OSTP request for information. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/national-priorities-for-artificial-intelligence-ostp-response>

- Smith, E. T., **Schuett, J.**, Anderljung, M., & Heim, L. (2023). Response to the NTIA AI accountability policy request for comment. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy>

- **Schuett, J.**, & Anderljung, M. (2022). Submission to the NIST AI Risk Management Framework. *Centre for the Governance of AI*. <https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework>

---

## Contact
Find me on [Twitter](https://twitter.com/jonasschuett) and [LinkedIn](https://www.linkedin.com/in/jonasschuett) or reach out to me at [jonas.schuett@governance.ai](mailto:jonas.schuett@governance.ai).

Find my research on [ORCID](https://orcid.org/0000-0001-7154-5049), [Google Scholar](https://scholar.google.com/citations?user=iZXltDgAAAAJ&hl=en&oi=ao), and [SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=3705327).
